---
title: "Final_presentation_OwnDataset"
author: "Raj Motwani"
date: "2024-04-28"
output: html_document
---


```{r}
library(cluster)
library(readr)
library(factoextra)
library(magrittr)
library(NbClust)
library(readr)
#library(MVA)
library(HSAUR2)
library(SciViews)
library(scatterplot3d)
library(car)
library(lattice)
library(GGally)
library(ggplot2)
library(ggridges)
library(ggvis)
library(ggthemes)
library(cowplot)
library(gapminder)
library(gganimate)
library(dplyr)
library(tidyverse)
library(grid)
library(gridExtra)
library(RColorBrewer)
library(Hotelling)
library(stats)
library(biotools)
library(factoextra)
library(FactoMineR)
library(ggfortify)
library(psych)
library(corrplot)
library(devtools)
library(cluster)
library(magrittr)
library(NbClust)
library(MASS)
library(gvlma)
library(leaps)
library(relaimpo)
library(e1071)
library(pROC)
library(memisc)
library(ROCR)
library(klaR)
library(caret)
library(caTools)

# With made up data. 
b_data <- read_csv("/Users/raj/Desktop/Billionaires Statistics Dataset.csv")
b_data


# Identify numeric columns
numeric_cols <- sapply(b_data, is.numeric)

# Subset data to include only numeric columns
numeric_data <- b_data[, numeric_cols]

# Calculate correlation matrix
cor <- cor(numeric_data)

# Plot correlation matrix
corrplot(cor, type = "upper", method = "color")
```
The code performs PCA on the dataset "b_data" with different subsets of variables. The first PCA (b_pca) considers only the age, final worth, birth year, and birth month, while the second PCA (b_new_pca) includes all variables except the rank. Scaling is applied in both cases, ensuring that each variable contributes equally to the analysis regardless of its scale.
```{r }
shortened_b <- b_data[,c(0:6)]
shortened_b
b_pca <- prcomp(shortened_b[,1:4],scale=TRUE)
b_new_pca <- prcomp(shortened_b[,-1],scale=TRUE)
b_new_pca


#The bars in the graph represent the proportion of variance explained by each principal component. For example, the first principal component (PC1) explains 50.5% of the variance in the data. The second principal component (PC2) explains 41.8% of the variance, and so on.
fviz_eig(b_pca, addlabels = TRUE)



#The elbow in the scree plot is a common way to decide how many principal components to keep. In this case, you might decide to keep the first two principal components (PC1 and PC2) because they capture a significant amount of the variance (over 60%) and the variance explained by subsequent components starts to tail off.
fviz_eig(b_new_pca, addlabels = TRUE)

#both the scree plot have over 60% variance so we will do EFA 



```
```{r }

fit.b <- principal(b_data[,1:6], nfactors=3, rotate="varimax")
fit.b
fa.diagram(fit.b)
```

```{r }
attach(b_data)
str(b_data)
excluded_columns <- c(-4,-5,-6,-7,-8,-9,-10,-11,-12,-13,-14,-15,-16,-17,-18,-19,-20,-21,-22)
result <- cor(b_data[,excluded_columns])
excluded_columns
result
b_data_pca <- prcomp(b_data[,excluded_columns],scale=TRUE)
b_data_pca
summary(b_data_pca)
# sample scores stored 
(eigen_b_data <- b_data_pca$sdev^2)
names(eigen_b_data) <- paste("PC",1:3,sep="")
eigen_b_data


fviz_eig(b_data_pca, addlabels = TRUE)
fviz_pca_var(b_data_pca,col.var = "cos2",
             gradient.cols = c("#FFCC00", "#CC9933", "#660033", "#330033"),
             repel = TRUE)


```
```{r }
base_data <- read.csv("/Users/raj/Desktop/Compressed_b_data.csv",row.names=1, fill = TRUE)
base_data

(agn.base_data <- agnes(base_data, metric="euclidean", stand=TRUE, method = "single"))
plot(agn.base_data, which.plots=1)
plot(agn.base_data, which.plots=2)
plot(agn.base_data, which.plots=3)
matstd.base_data <- scale(base_data[,2:6])
matstd.base_data
(kmeans2.base_data <- kmeans(matstd.base_data,2,nstart =10))
# Computing the percentage of variation accounted for. Two clusters
perc.var.2 <- round(100*(1 - kmeans2.base_data$betweenss/kmeans2.base_data$totss),1)
names(perc.var.2) <- "Perc. 2 clus"
perc.var.2

# Computing the percentage of variation accounted for. Three clusters
(kmeans3.base_data <- kmeans(matstd.base_data,3,nstart =10))
perc.var.3 <- round(100*(1 - kmeans3.base_data$betweenss/kmeans3.base_data$totss),1)
names(perc.var.3) <- "Perc. 3 clus"
perc.var.3

# Computing the percentage of variation accounted for. Four clusters
(kmeans4.base_data <- kmeans(matstd.base_data,4,nstart = 10))
perc.var.4 <- round(100*(1 - kmeans4.base_data$betweenss/kmeans4.base_data$totss),1)
names(perc.var.4) <- "Perc. 4 clus"
perc.var.4

# Computing the percentage of variation accounted for. Five clusters
(kmeans5.base_data <- kmeans(matstd.base_data,5,nstart = 10))
perc.var.5 <- round(100*(1 - kmeans5.base_data$betweenss/kmeans5.base_data$totss),1)
names(perc.var.5) <- "Perc. 5 clus"
perc.var.5

cluster_assignments <- kmeans4.base_data$cluster
Variance_List <- c(perc.var.2,perc.var.3,perc.var.4,perc.var.5)
Variance_List
plot(Variance_List)
#
# Saving four k-means clusters in a list
clus1 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 1]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 1]))
colnames(clus1) <- "Cluster 1"


clus2 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 2]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 2]))
colnames(clus2) <- "Cluster 2"
clus3 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 3]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 3]))
colnames(clus3) <- "Cluster 3"
clus4 <- matrix(names(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 4]), 
                ncol=1, nrow=length(kmeans4.base_data$cluster[kmeans4.base_data$cluster == 4]))
colnames(clus4) <- "Cluster 4"
list(clus1,clus2,clus3,clus4)


df_with_clusters <- cbind( Cluster = kmeans4.base_data$cluster)
# Display the data frame with cluster assignments
head(df_with_clusters)
# Assuming 'df' is your original data frame
df_with_clusters <- cbind( Cluster = cluster_assignments)
# Perform PCA
pca_result <- prcomp(matstd.base_data)

# Extract the first two principal components
pc1 <- pca_result$x[, 1]
pc2 <- pca_result$x[, 2]

# Scatter plot with color-coded clusters
plot(pc1, pc2, col = cluster_assignments, pch = 16, main = "PCA of Clusters", xlab = "PC1", ylab = "PC2")
legend("topright", legend = unique(cluster_assignments), col = unique(cluster_assignments), pch = 16, title = "Cluster")


```
#PCA is a linear transformation, and it may not be able to capture the full complexity of non-linear relationships between the data points.Hence, it is not properly classified.


LOGISTIC REGRESSION
```{r }
str(b_data)
colnames(b_data)


# Factorize categorical 
b_data$gender <- as.factor(b_data$gender)
b_data$selfMade <- as.factor(b_data$selfMade)


# Split the data into training and testing sets
set.seed(123) # for reproducibility
train_index <- sample(1:nrow(b_data), 0.7 * nrow(b_data))
train_data <- b_data[train_index, ]
test_data <- b_data[-train_index, ]

# Build the logistic regression model
model <- glm(gender ~ age + finalWorth + selfMade, data = train_data, family = binomial)

# Summary of the model
summary(model)



```

```{r }
# Assess model performance on the test set
predicted <- predict(model, test_data, type = "response")
predicted

# Create a dataframe containing predicted values and observed values (gender)
prediction_data <- data.frame(predicted = predicted, observed = test_data$gender)
# Re-run prediction

# ROC curve
roc_obj <- roc(prediction_data$observed, prediction_data$predicted)
plot(roc_obj)


predictions <- predict(model, newdata = test_data, type = "response")

# Convert predicted probabilities to binary predictions
binary_predictions <- ifelse(predictions > 0.1, 1, 0)


# Residual analysis
# Plot residuals vs fitted values
plot(resid(model) ~ fitted(model), main = "Residuals vs Fitted Values")

# Plot Q-Q plot of residuals
qqnorm(resid(model))
qqline(resid(model))

# Plot predictions
plot(test_data$gender, predictions, main = "Actual vs Predicted", xlab = "Actual", ylab = "Predicted")
abline(0, 1, col = "red")
#binary_predictions <- as.factor(binary_predictions)
accuracy <- mean(binary_predictions == test_data$gender)
accuracy

```
#Center line: The horizontal line in the middle of the box represents the median predicted value. It divides the data into two halves, with half the predictions having values higher than the median and the other half having values lower.
#Box: The box represents the interquartile range (IQR), which encompasses the middle 50% of the predicted values. The bottom edge of the box corresponds to the first quartile (Q1) and the top edge to the third quartile (Q3).
